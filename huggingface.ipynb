{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip install -U -q transformers bitsandbytes"
      ],
      "metadata": {
        "id": "1kSbNsUR1VNn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from huggingface_hub import login\n",
        "login()"
      ],
      "metadata": {
        "id": "SNW_XtAO2npJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import transformers\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
        "\n",
        "# quantization allows to run larger model on small household GPUs\n",
        "bnb_config = transformers.BitsAndBytesConfig(\n",
        "load_in_4bit=True,\n",
        "bnb_4bit_use_double_quant=True,\n",
        "bnb_4bit_quant_type=\"nf4\",\n",
        "bnb_4bit_compute_dtype=torch.bfloat16\n",
        ")"
      ],
      "metadata": {
        "id": "fkopxcYO2IsC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## LLMs generates sentence autoregressively\n",
        "\n",
        "We use GPT2 as an example."
      ],
      "metadata": {
        "id": "e13cfKCy3nXT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model = AutoModelForCausalLM.from_pretrained(\"gpt2\")\n",
        "tokenizer = AutoTokenizer.from_pretrained(\n",
        "    \"gpt2\",\n",
        "    trust_remote_code=True,\n",
        "    quantization_config=bnb_config,\n",
        "    device_map='auto',\n",
        ")\n",
        "\n",
        "print(model)"
      ],
      "metadata": {
        "id": "1C_L780S33w-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Tokenizer and embedding\n",
        "\n",
        "Tokenizer transforms sentence in English to integers.\n",
        "\n",
        "Embedding is a huge table where integers are converted to their indexed entries."
      ],
      "metadata": {
        "id": "pTNncAzd4S4Z"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "# Tokenize some input text\n",
        "model_inputs = tokenizer(\"Do you love me?\", return_tensors=\"pt\")\n",
        "\n",
        "print(model_inputs)\n",
        "print(model.transformer.wte(model_inputs['input_ids']))\n"
      ],
      "metadata": {
        "id": "pTlLJmRj4KTK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.generation_config.pad_token_id = tokenizer.pad_token_id\n",
        "generated_ids = model.generate(**model_inputs, max_new_tokens=10, do_sample=False)\n",
        "decoded = tokenizer.batch_decode(generated_ids)\n",
        "print(decoded[0])"
      ],
      "metadata": {
        "id": "Br9VdgzQ4I4Z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print('Generated tokens', generated_ids[0,model_inputs['input_ids'].shape[1]:])"
      ],
      "metadata": {
        "id": "Rx-muXqJ0YTY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import copy\n",
        "\n",
        "new_model_inputs = copy.copy(model_inputs)"
      ],
      "metadata": {
        "id": "TEN3iCQ92IW1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Under the hood, the model predicts the probability of the next token, and the highest in this case is chosen to be the output.\n",
        "\n",
        "The chosen token is concatenated to the input integer list, and the process goes on until the model predict the End-of-Sentence token."
      ],
      "metadata": {
        "id": "58oYWEsl4nlx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "for i in range(10):\n",
        "\n",
        "    highest_prob_token = torch.argmax(model(**new_model_inputs)['logits'][0,-1])\n",
        "    print('Tokens with highest probability', highest_prob_token)\n",
        "    print(tokenizer.batch_decode(highest_prob_token.reshape(1,-1)))\n",
        "\n",
        "    new_model_inputs = dict(\n",
        "        input_ids = torch.cat([new_model_inputs['input_ids'], highest_prob_token.reshape(1,1)], axis=1),\n",
        "    )\n",
        "\n",
        "    new_model_inputs['attention_mask'] = torch.ones_like(new_model_inputs['input_ids'])\n",
        "\n"
      ],
      "metadata": {
        "id": "a6x6GTHxzsdX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "new_model_inputs = dict(\n",
        "    input_ids = torch.cat([model_inputs['input_ids'], highest_prob_token.reshape(1,1)], axis=1),\n",
        ")\n",
        "\n",
        "new_model_inputs['attention_mask'] = torch.ones_like(new_model_inputs['input_ids'])\n"
      ],
      "metadata": {
        "id": "8lT6-iw60T_g"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "next_highest_prob_token = torch.argmax(model(**new_model_inputs)['logits'][0,-1])\n",
        "print('Tokens with highest probability', next_highest_prob_token)\n",
        "print(tokenizer.batch_decode(next_highest_prob_token.reshape(1,-1)))"
      ],
      "metadata": {
        "id": "gbEugMRI0HV0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "N6f4_k0XwnCD"
      },
      "outputs": [],
      "source": [
        "\n",
        "\n",
        "model_id = \"mistralai/Mistral-7B-Instruct-v0.1\"\n",
        "\n",
        "\n",
        "model = transformers.AutoModelForCausalLM.from_pretrained(\n",
        "model_id,\n",
        "trust_remote_code=True,\n",
        "quantization_config=bnb_config,\n",
        "device_map='auto',\n",
        ")\n",
        "\n",
        "tokenizer = transformers.AutoTokenizer.from_pretrained(\n",
        "model_id,\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "text = \"[INST] Do you love me? [/INST]\"\n",
        "\n",
        "tokenizer = transformers.AutoTokenizer.from_pretrained(\n",
        "model_id,\n",
        ")\n",
        "\n",
        "encodeds = tokenizer(text, return_tensors=\"pt\", add_special_tokens=False)\n",
        "model_inputs = encodeds.to('cuda:0')\n",
        "model.generation_config.pad_token_id = tokenizer.pad_token_id\n",
        "generated_ids = model.generate(**model_inputs, max_new_tokens=200, do_sample=True)\n",
        "decoded = tokenizer.batch_decode(generated_ids)\n",
        "print(decoded[0])"
      ],
      "metadata": {
        "id": "SanPKPjexFZw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "encodeds['input_ids']"
      ],
      "metadata": {
        "id": "eRELTq4VDiPa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.model.embed_tokens(encodeds['input_ids']).shape"
      ],
      "metadata": {
        "id": "IynPXO4UDX6I"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}